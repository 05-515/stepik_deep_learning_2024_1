{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 20320,
          "databundleVersionId": 1134053,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30683,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/05-515/stepik_deep_learning_2024_1/blob/main/14_to_stepik_simpsons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание. Классификация изображений\n",
        "\n",
        "Сегодня вам предстоить помочь телекомпании FOX в обработке их контента. Как вы знаете, сериал \"Симпсоны\" идет на телеэкранах более 25 лет, и за это время скопилось очень много видеоматериала. Персоонажи менялись вместе с изменяющимися графическими технологиями, и Гомер Симпсон-2018 не очень похож на Гомера Симпсона-1989. В этом задании вам необходимо классифицировать персонажей, проживающих в Спрингфилде. Думаю, нет смысла представлять каждого из них в отдельности.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xw7YkEefehWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установка зависимостей"
      ],
      "metadata": {
        "id": "oG47vhLxKNln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torchvision"
      ],
      "metadata": {
        "id": "_ITX8q-BMkKu",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:00.382933Z",
          "iopub.execute_input": "2024-04-15T19:52:00.383283Z",
          "iopub.status.idle": "2024-04-15T19:52:27.526303Z",
          "shell.execute_reply.started": "2024-04-15T19:52:00.383254Z",
          "shell.execute_reply": "2024-04-15T19:52:27.525014Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "from torchvision import transforms\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "from matplotlib import colors, pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "metadata": {
        "id": "WWgcwKwCLBfr",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:27.530641Z",
          "iopub.execute_input": "2024-04-15T19:52:27.530942Z",
          "iopub.status.idle": "2024-04-15T19:52:40.75433Z",
          "shell.execute_reply.started": "2024-04-15T19:52:27.530914Z",
          "shell.execute_reply": "2024-04-15T19:52:40.752887Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# разные режимы датасета\n",
        "DATA_MODES = ['train', 'val', 'test']\n",
        "# все изображения будут масштабированы к размеру 224x224 px\n",
        "RESCALE_SIZE = 224\n",
        "# работаем на видеокарте\n",
        "DEVICE = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "WTdzMtgJP15N",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:40.755495Z",
          "iopub.execute_input": "2024-04-15T19:52:40.755881Z",
          "iopub.status.idle": "2024-04-15T19:52:40.760671Z",
          "shell.execute_reply.started": "2024-04-15T19:52:40.755858Z",
          "shell.execute_reply": "2024-04-15T19:52:40.759504Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://jhui.github.io/2018/02/09/PyTorch-Data-loading-preprocess_torchvision/\n"
      ],
      "metadata": {
        "id": "HYFeKUzfy572"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ниже мы исспользуем враппер над датасетом для удобной работы. Вам стоит понимать, что происходит с LabelEncoder и  с torch.Transformation.\n",
        "\n",
        "ToTensor конвертирует  PIL Image с параметрами в диапазоне [0, 255] (как все пиксели) в FloatTensor размера (C x H x W) [0,1] , затем производится масштабирование:\n",
        "$input = \\frac{input - \\mu}{\\text{standard deviation}} $, <br>       константы - средние и дисперсии по каналам на основе ImageNet\n",
        "\n",
        "\n",
        "Стоит также отметить, что мы переопределяем метод __getitem__ для удобства работы с данной структурой данных.\n",
        " Также используется LabelEncoder для преобразования строковых меток классов в id и обратно. В описании датасета указано, что картинки разного размера, так как брались напрямую с видео, поэтому следуем привести их к одному размер (это делает метод  _prepare_sample)\n",
        "\n",
        "Тут я добавил различные аугментации. Для обучающей выборки - это RandomHorizontalFlip, RandomAutocontrast, GaussianBlur и Normalize. А для валидации и теста только Normalize."
      ],
      "metadata": {
        "id": "8ecnkB2xK1aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpsonsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Датасет с картинками, который паралельно подгружает их из папок\n",
        "    производит скалирование и превращение в торчевые тензоры\n",
        "    \"\"\"\n",
        "    def __init__(self, files, mode):\n",
        "        super().__init__()\n",
        "        # список файлов для загрузки\n",
        "        self.files = sorted(files)\n",
        "        # режим работы\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode not in DATA_MODES:\n",
        "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
        "            raise NameError\n",
        "\n",
        "        self.len_ = len(self.files)\n",
        "\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        if self.mode != 'test':\n",
        "            self.labels = [path.parent.name for path in self.files]\n",
        "            self.label_encoder.fit(self.labels)\n",
        "\n",
        "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
        "                  pickle.dump(self.label_encoder, le_dump_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len_\n",
        "\n",
        "    def load_sample(self, file):\n",
        "        image = Image.open(file)\n",
        "        image.load()\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # для преобразования изображений в тензоры PyTorch и нормализации входа\n",
        "        if self.mode == 'train':\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomAutocontrast(),\n",
        "                transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        x = self.load_sample(self.files[index])\n",
        "        x = self._prepare_sample(x)\n",
        "        x = np.array(x / 255, dtype='float32')\n",
        "        x = transform(x)\n",
        "        if self.mode == 'test':\n",
        "            return x\n",
        "        else:\n",
        "            label = self.labels[index]\n",
        "            label_id = self.label_encoder.transform([label])\n",
        "            y = label_id.item()\n",
        "            return x, y\n",
        "\n",
        "    def _prepare_sample(self, image):\n",
        "        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
        "        return np.array(image)"
      ],
      "metadata": {
        "id": "cj32U5iTQUe4",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:40.76497Z",
          "iopub.execute_input": "2024-04-15T19:52:40.76531Z",
          "iopub.status.idle": "2024-04-15T19:52:40.781703Z",
          "shell.execute_reply.started": "2024-04-15T19:52:40.765281Z",
          "shell.execute_reply": "2024-04-15T19:52:40.78076Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(inp, title=None, plt_ax=plt, default=False):\n",
        "    \"\"\"Imshow для тензоров\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt_ax.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt_ax.set_title(title)\n",
        "    plt_ax.grid(False)"
      ],
      "metadata": {
        "id": "j_odtTEzcaWH",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:40.783069Z",
          "iopub.execute_input": "2024-04-15T19:52:40.783683Z",
          "iopub.status.idle": "2024-04-15T19:52:40.799102Z",
          "shell.execute_reply.started": "2024-04-15T19:52:40.783647Z",
          "shell.execute_reply": "2024-04-15T19:52:40.798122Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = Path('/kaggle/input/journey-springfield/train/simpsons_dataset')\n",
        "TEST_DIR = Path('/kaggle/input/journey-springfield/testset/testset')\n",
        "\n",
        "train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n",
        "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))"
      ],
      "metadata": {
        "id": "yUhzOq1zRJil",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:40.800549Z",
          "iopub.execute_input": "2024-04-15T19:52:40.800898Z",
          "iopub.status.idle": "2024-04-15T19:52:50.094136Z",
          "shell.execute_reply.started": "2024-04-15T19:52:40.800863Z",
          "shell.execute_reply": "2024-04-15T19:52:50.093344Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_labels = [path.parent.name for path in train_val_files]\n",
        "train_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n",
        "                                          stratify=train_val_labels)"
      ],
      "metadata": {
        "id": "TmPhhKKlRyCF",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:50.095519Z",
          "iopub.execute_input": "2024-04-15T19:52:50.095778Z",
          "iopub.status.idle": "2024-04-15T19:52:50.164675Z",
          "shell.execute_reply.started": "2024-04-15T19:52:50.095755Z",
          "shell.execute_reply": "2024-04-15T19:52:50.164002Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = SimpsonsDataset(val_files, mode='val')\n",
        "train_dataset = SimpsonsDataset(train_files, mode='train')"
      ],
      "metadata": {
        "id": "aAimOLjSQGTh",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:50.165629Z",
          "iopub.execute_input": "2024-04-15T19:52:50.165864Z",
          "iopub.status.idle": "2024-04-15T19:52:50.398577Z",
          "shell.execute_reply.started": "2024-04-15T19:52:50.165843Z",
          "shell.execute_reply": "2024-04-15T19:52:50.397681Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте посмотрим на наших героев внутри датасета."
      ],
      "metadata": {
        "id": "PmKSdyv1b7PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(8, 8), \\\n",
        "                        sharey=True, sharex=True)\n",
        "for fig_x in ax.flatten():\n",
        "    random_characters = int(np.random.uniform(0,1000))\n",
        "    im_val, label = val_dataset[random_characters]\n",
        "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
        "                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n",
        "    imshow(im_val.data.cpu(), \\\n",
        "          title=img_label,plt_ax=fig_x)"
      ],
      "metadata": {
        "id": "ltitWp3lXAZt",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:50.399665Z",
          "iopub.execute_input": "2024-04-15T19:52:50.399931Z",
          "iopub.status.idle": "2024-04-15T19:52:52.504021Z",
          "shell.execute_reply.started": "2024-04-15T19:52:50.399907Z",
          "shell.execute_reply": "2024-04-15T19:52:52.503109Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(np.unique(train_val_labels))\n",
        "print(\"Кол-во классов :{}\".format(n_classes))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:52.507399Z",
          "iopub.execute_input": "2024-04-15T19:52:52.507687Z",
          "iopub.status.idle": "2024-04-15T19:52:52.522196Z",
          "shell.execute_reply.started": "2024-04-15T19:52:52.507661Z",
          "shell.execute_reply": "2024-04-15T19:52:52.521278Z"
        },
        "trusted": true,
        "id": "NYdVw0R5B4WV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Построение нейросети\n",
        "\n",
        "Делать модель с нуля будет весьма труднозатратно и не гарантирует хороший результат, поэтому воспользуемся всемогущим Transfer Learning.\n"
      ],
      "metadata": {
        "id": "u6YcZk8vQR47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на кол-во обучающих данных и число классов."
      ],
      "metadata": {
        "id": "FpTY6QOsB4WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(np.unique(train_val_labels))\n",
        "print(\"Кол-во классов: {}\".format(n_classes))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:52.523251Z",
          "iopub.execute_input": "2024-04-15T19:52:52.523507Z",
          "iopub.status.idle": "2024-04-15T19:52:52.54034Z",
          "shell.execute_reply.started": "2024-04-15T19:52:52.523485Z",
          "shell.execute_reply": "2024-04-15T19:52:52.539461Z"
        },
        "trusted": true,
        "id": "FL8z3hBpB4WW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Размер обучающей выборки: {}\".format(len(train_files)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:52.54135Z",
          "iopub.execute_input": "2024-04-15T19:52:52.541592Z",
          "iopub.status.idle": "2024-04-15T19:52:52.54709Z",
          "shell.execute_reply.started": "2024-04-15T19:52:52.541571Z",
          "shell.execute_reply": "2024-04-15T19:52:52.546274Z"
        },
        "trusted": true,
        "id": "aDbYVSXbB4WX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Воспользуемся ResNet50, а чтобы все работало корректно, поменяем последний слой.\n",
        "\n",
        "Данных для обучения больше 10к, но они сильно отличаются от ImageNet. Поэтому попробуем зафайнтюнить всю сеть."
      ],
      "metadata": {
        "id": "ebNH92UTB4WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet50(weights=\"IMAGENET1K_V2\")"
      ],
      "metadata": {
        "id": "1PJcWAhuji-i",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:52.548288Z",
          "iopub.execute_input": "2024-04-15T19:52:52.548558Z",
          "iopub.status.idle": "2024-04-15T19:52:53.959487Z",
          "shell.execute_reply.started": "2024-04-15T19:52:52.548535Z",
          "shell.execute_reply": "2024-04-15T19:52:53.958623Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:53.961017Z",
          "iopub.execute_input": "2024-04-15T19:52:53.961398Z",
          "iopub.status.idle": "2024-04-15T19:52:53.967702Z",
          "shell.execute_reply.started": "2024-04-15T19:52:53.961364Z",
          "shell.execute_reply": "2024-04-15T19:52:53.966823Z"
        },
        "trusted": true,
        "id": "ZfgBasulB4WY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc = nn.Linear(2048, n_classes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:53.968861Z",
          "iopub.execute_input": "2024-04-15T19:52:53.969209Z",
          "iopub.status.idle": "2024-04-15T19:52:53.979231Z",
          "shell.execute_reply.started": "2024-04-15T19:52:53.969177Z",
          "shell.execute_reply": "2024-04-15T19:52:53.978473Z"
        },
        "trusted": true,
        "id": "Uhb0JQp7B4WY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_epoch(model, train_loader, criterion, optimizer):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_data = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        processed_data += inputs.size(0)\n",
        "\n",
        "    train_loss = running_loss / processed_data\n",
        "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "e2mk7MNtcUhJ",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:53.980154Z",
          "iopub.execute_input": "2024-04-15T19:52:53.980374Z",
          "iopub.status.idle": "2024-04-15T19:52:53.990158Z",
          "shell.execute_reply.started": "2024-04-15T19:52:53.980355Z",
          "shell.execute_reply": "2024-04-15T19:52:53.989185Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_size = 0\n",
        "\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        processed_size += inputs.size(0)\n",
        "    val_loss = running_loss / processed_size\n",
        "    val_acc = running_corrects.double() / processed_size\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "w_CD9--hcUjs",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:53.991279Z",
          "iopub.execute_input": "2024-04-15T19:52:53.99156Z",
          "iopub.status.idle": "2024-04-15T19:52:54.006591Z",
          "shell.execute_reply.started": "2024-04-15T19:52:53.991536Z",
          "shell.execute_reply": "2024-04-15T19:52:54.005736Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_files, val_files, model, epochs, batch_size):\n",
        "    train_loader = DataLoader(train_files, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_files, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    history = []\n",
        "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
        "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
        "\n",
        "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=4, gamma=0.1)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
        "            print(\"loss\", train_loss)\n",
        "            scheduler.step()\n",
        "\n",
        "            model.eval()\n",
        "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "            pbar_outer.update(1)\n",
        "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
        "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "NaxYIwB3cUmX",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:54.007728Z",
          "iopub.execute_input": "2024-04-15T19:52:54.008016Z",
          "iopub.status.idle": "2024-04-15T19:52:54.020303Z",
          "shell.execute_reply.started": "2024-04-15T19:52:54.00799Z",
          "shell.execute_reply": "2024-04-15T19:52:54.019478Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_loader):\n",
        "    with torch.inference_mode():\n",
        "        logits = []\n",
        "\n",
        "        for inputs in test_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            model.eval()\n",
        "            outputs = model(inputs).cpu()\n",
        "            logits.append(outputs)\n",
        "\n",
        "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
        "    return probs"
      ],
      "metadata": {
        "id": "v6G7qbYqcUpL",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:54.021376Z",
          "iopub.execute_input": "2024-04-15T19:52:54.021658Z",
          "iopub.status.idle": "2024-04-15T19:52:54.035658Z",
          "shell.execute_reply.started": "2024-04-15T19:52:54.021634Z",
          "shell.execute_reply": "2024-04-15T19:52:54.034899Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(np.unique(train_val_labels))\n",
        "simple_cnn = model.to(DEVICE)\n",
        "print(\"we will classify :{}\".format(n_classes))\n",
        "print(model)"
      ],
      "metadata": {
        "id": "yzwhB4K3dQOC",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:54.036576Z",
          "iopub.execute_input": "2024-04-15T19:52:54.036829Z",
          "iopub.status.idle": "2024-04-15T19:52:54.292555Z",
          "shell.execute_reply.started": "2024-04-15T19:52:54.036808Z",
          "shell.execute_reply": "2024-04-15T19:52:54.291575Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запустим обучение сети."
      ],
      "metadata": {
        "id": "bo3UND5RdgVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if val_dataset is None:\n",
        "    val_dataset = SimpsonsDataset(val_files, mode='val')\n",
        "\n",
        "train_dataset = SimpsonsDataset(train_files, mode='train')"
      ],
      "metadata": {
        "id": "WDkcxZ1kfD4a",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:54.293732Z",
          "iopub.execute_input": "2024-04-15T19:52:54.293997Z",
          "iopub.status.idle": "2024-04-15T19:52:54.478693Z",
          "shell.execute_reply.started": "2024-04-15T19:52:54.293974Z",
          "shell.execute_reply": "2024-04-15T19:52:54.477815Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "history = train(train_dataset, val_dataset, model=model, epochs=10, batch_size=128)"
      ],
      "metadata": {
        "id": "iDXoR8PIdfLD",
        "execution": {
          "iopub.status.busy": "2024-04-15T19:52:54.47975Z",
          "iopub.execute_input": "2024-04-15T19:52:54.48001Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим кривые обучения"
      ],
      "metadata": {
        "id": "7qMAdL_BduXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc, val_loss, val_acc = zip(*history)"
      ],
      "metadata": {
        "id": "2ryD_9yFdfNr",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(loss, label=\"train_loss\")\n",
        "plt.plot(val_loss, label=\"val_loss\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GpQDWGkfdfQ5",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_one_sample(model, inputs, device=DEVICE):\n",
        "    \"\"\"Предсказание, для одной картинки\"\"\"\n",
        "    with torch.inference_mode():\n",
        "        inputs = inputs.to(device)\n",
        "        model.eval()\n",
        "        logit = model(inputs).cpu()\n",
        "        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n",
        "    return probs"
      ],
      "metadata": {
        "id": "Z8PlF6o0N9O1",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "random_characters = int(np.random.uniform(0,1000))\n",
        "ex_img, true_label = val_dataset[random_characters]\n",
        "probs_im = predict_one_sample(simple_cnn, ex_img.unsqueeze(0))"
      ],
      "metadata": {
        "id": "pY_OoLoVO_9V",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = list(map(int, np.random.uniform(0,1000, 20)))\n",
        "imgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n",
        "\n",
        "probs_ims = predict(simple_cnn, imgs)"
      ],
      "metadata": {
        "id": "caivVFeAN9SY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))"
      ],
      "metadata": {
        "id": "t-0pRdHnQQKM",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(probs_ims,-1)\n",
        "\n",
        "actual_labels = [val_dataset[id][1] for id in idxs]\n",
        "\n",
        "preds_class = [label_encoder.classes_[i] for i in y_pred]"
      ],
      "metadata": {
        "id": "GNMFc7sfQh1a",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделаем классную визуализацию,  чтобы посмотреть насколько сеть уверена в своих ответах. Можете исспользовать это, чтобы отлаживать правильность вывода."
      ],
      "metadata": {
        "id": "pxB9pfPfdCHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as patches\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n",
        "                        sharey=True, sharex=True)\n",
        "for fig_x in ax.flatten():\n",
        "    random_characters = int(np.random.uniform(0,1000))\n",
        "    im_val, label = val_dataset[random_characters]\n",
        "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
        "                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n",
        "\n",
        "\n",
        "\n",
        "    imshow(im_val.data.cpu(), \\\n",
        "          title=img_label,plt_ax=fig_x)\n",
        "\n",
        "    actual_text = \"Actual : {}\".format(img_label)\n",
        "\n",
        "    fig_x.add_patch(patches.Rectangle((0, 53),86,35,color='white'))\n",
        "    font0 = FontProperties()\n",
        "    font = font0.copy()\n",
        "    font.set_family(\"fantasy\")\n",
        "    prob_pred = predict_one_sample(simple_cnn, im_val.unsqueeze(0))\n",
        "    predicted_proba = np.max(prob_pred)*100\n",
        "    y_pred = np.argmax(prob_pred)\n",
        "\n",
        "    predicted_label = label_encoder.classes_[y_pred]\n",
        "    predicted_label = predicted_label[:len(predicted_label)//2] + '\\n' + predicted_label[len(predicted_label)//2:]\n",
        "    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n",
        "\n",
        "    fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n",
        "                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')"
      ],
      "metadata": {
        "id": "VVjq4EC5ZZE7",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit на Kaggle"
      ],
      "metadata": {
        "id": "QEWTL6jgdh7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://i.redd.it/nuaphfioz0211.jpg)"
      ],
      "metadata": {
        "id": "wrjQ6cxHIGtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SimpsonsDataset(test_files, mode=\"test\")\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
        "probs = predict(simple_cnn, test_loader)\n",
        "\n",
        "\n",
        "preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
        "test_filenames = [path.name for path in test_dataset.files]\n"
      ],
      "metadata": {
        "id": "9UTbU0Zbc6Hb",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "my_submit = pd.read_csv(\"/kaggle/input/journey-springfield/sample_submission.csv\")\n",
        "# my_submit = pd.DataFrame({'Image_id': test_filenames, 'Expected': preds})\n",
        "my_submit.head()"
      ],
      "metadata": {
        "id": "yw0zZ-Hdd89s",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "my_submission = pd.DataFrame({'Id': test_filenames, 'Expected': preds})"
      ],
      "metadata": {
        "id": "VIYaqa20iYTL",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "my_submission.to_csv('/kaggle/working/my_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "5rdlyMKtiYe2",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Score: 0.99362**"
      ],
      "metadata": {
        "id": "Blhmmhj0B4Wf"
      }
    }
  ]
}